// Copyright (c) SimpleStaking, Viable Systems and Tezedge Contributors
// SPDX-License-Identifier: MIT

use std::cell::RefCell;
use std::path::PathBuf;
use std::sync::{Arc, Condvar, Mutex};
use std::time::Duration;

use lazy_static::lazy_static;
use slog::{debug, Logger};
use tezos_context_api::{
    ContextKeyOwned, ContextValue, PatchContext, StringTreeObject, TezosContextStorageConfiguration,
};
use thiserror::Error;

use crypto::hash::{ChainId, ContextHash, ProtocolHash};
use ipc::*;
use tezos_api::environment::TezosEnvironmentConfiguration;
use tezos_api::ffi::*;
use tezos_messages::p2p::encoding::operation::Operation;

use tezos_protocol_ipc_messages::*;

use crate::runner::{ExecutableProtocolRunner, ProtocolRunnerError};
use crate::ProtocolEndpointConfiguration;

lazy_static! {
    /// Ww need to have multiple multiple FFI runtimes, and runtime needs to have initialized protocol context,
    /// but there are some limitations,
    /// e.g.: if we start readonly context with empty context directory, it fails in FFI runtime in irmin initialization,
    /// so we need to control this initialization on application level,
    ///
    /// in application we can have multiple threads, which tries to call init_protocol (read or write),
    /// so this lock ensures, that in the whole application at least one 'write init_protocol_context' was successfull, which means,
    /// that FFI context has created required files, so after that we can let continue other threads to initialize readonly context
    ///
    /// see also: test_multiple_protocol_runners_with_one_write_multiple_read_init_context
    static ref AT_LEAST_ONE_WRITE_PROTOCOL_CONTEXT_WAS_SUCCESS_AT_FIRST_LOCK: Arc<(Mutex<bool>, Condvar)> = Arc::new((Mutex::new(false), Condvar::new()));
}

/// Error types generated by a tezos protocol.
#[derive(Error, Debug)]
pub enum ProtocolError {
    /// Protocol rejected to apply a block.
    #[error("Apply block error: {reason}")]
    ApplyBlockError { reason: ApplyBlockError },
    #[error("Assert encoding for protocol data error: {reason}")]
    AssertEncodingForProtocolDataError { reason: ProtocolDataError },
    #[error("Begin construction error: {reason}")]
    BeginApplicationError { reason: BeginApplicationError },
    #[error("Begin construction error: {reason}")]
    BeginConstructionError { reason: BeginConstructionError },
    #[error("Validate operation error: {reason}")]
    ValidateOperationError { reason: ValidateOperationError },
    #[error("Protocol rpc call error: {reason}")]
    ProtocolRpcError {
        reason: ProtocolRpcError,
        request_path: String,
    },
    #[error("Helper Preapply call error: {reason}")]
    HelpersPreapplyError { reason: HelpersPreapplyError },
    #[error("Compute path call error: {reason}")]
    ComputePathError { reason: ComputePathError },
    /// Error in configuration.
    #[error("OCaml runtime configuration error: {reason}")]
    TezosRuntimeConfigurationError {
        reason: TezosRuntimeConfigurationError,
    },
    /// OCaml part failed to initialize tezos storage.
    #[error("OCaml storage init error: {reason}")]
    OcamlStorageInitError { reason: TezosStorageInitError },
    /// OCaml part failed to get genesis data.
    #[error("Failed to get genesis data: {reason}")]
    GenesisResultDataError { reason: GetDataError },
    #[error("Failed to decode binary data to json ({caller}): {reason}")]
    FfiJsonEncoderError {
        caller: String,
        reason: FfiJsonEncoderError,
    },

    #[error("Failed to get key from history: {reason}")]
    ContextGetKeyFromHistoryError { reason: String },
    #[error("Failed to get values by prefix: {reason}")]
    ContextGetKeyValuesByPrefixError { reason: String },
}

/// Errors generated by `protocol_runner`.
#[derive(Error, Debug)]
pub enum ProtocolServiceError {
    /// Generic IPC communication error. See `reason` for more details.
    #[error("IPC error: {reason}")]
    IpcError { reason: IpcError },
    /// Tezos protocol error.
    #[error("Protocol error: {reason}")]
    ProtocolError { reason: ProtocolError },
    /// Unexpected message was received from IPC channel
    #[error("Received unexpected message: {message}")]
    UnexpectedMessage { message: &'static str },
    /// Invalid data error
    #[error("Invalid data error: {message}")]
    InvalidDataError { message: String },
    /// Lock error
    #[error("Lock error: {message:?}")]
    LockPoisonError { message: String },
    /// Context IPC server error
    #[error("Context IPC server error: {message:?}")]
    ContextIpcServerError { message: String },
}

impl ProtocolServiceError {
    /// Checks if this is an IPC error produced by sequence of timeouts
    pub fn is_ipc_timeout_chain(&self) -> bool {
        matches!(
            self,
            Self::IpcError {
                reason: IpcError::DiscardMessageTimeout,
            }
        )
    }

    pub fn is_restart_required(&self) -> bool {
        match self {
            ProtocolServiceError::IpcError { .. }
            | ProtocolServiceError::UnexpectedMessage { .. }
            | ProtocolServiceError::ContextIpcServerError { .. } => true,
            _ => false,
        }
    }

    pub fn handle_protocol_service_error<LC: Fn(ProtocolServiceError)>(
        error: ProtocolServiceError,
        log_callback: LC,
    ) -> Result<(), ProtocolServiceError> {
        if error.is_restart_required() {
            // we need to refresh protocol runner endpoint, so propagate error
            Err(error)
        } else {
            // just log error
            log_callback(error);
            Ok(())
        }
    }
}

impl<T> From<std::sync::PoisonError<T>> for ProtocolServiceError {
    fn from(source: std::sync::PoisonError<T>) -> Self {
        Self::LockPoisonError {
            message: source.to_string(),
        }
    }
}

impl slog::Value for ProtocolServiceError {
    fn serialize(
        &self,
        _record: &slog::Record,
        key: slog::Key,
        serializer: &mut dyn slog::Serializer,
    ) -> slog::Result {
        serializer.emit_arguments(key, &format_args!("{}", self))
    }
}

impl From<IpcError> for ProtocolServiceError {
    fn from(error: IpcError) -> Self {
        ProtocolServiceError::IpcError { reason: error }
    }
}

impl From<ProtocolError> for ProtocolServiceError {
    fn from(error: ProtocolError) -> Self {
        ProtocolServiceError::ProtocolError { reason: error }
    }
}

/// IPC command server is listening for incoming IPC connections.
pub struct IpcCmdServer(
    IpcServer<NodeMessage, ProtocolMessage>,
    ProtocolEndpointConfiguration,
);

/// Difference between `IpcCmdServer` and `IpcEvtServer` is:
/// * `IpcCmdServer` is used to create IPC channel over which commands from node are transferred to the protocol runner.
/// * `IpcEvtServer` is used to create IPC channel over which events are transmitted from protocol runner to the tezedge node.
impl IpcCmdServer {
    const IO_TIMEOUT: Duration = Duration::from_secs(10);
    /// Long timeout for slow calls
    const IO_TIMEOUT_LONG: Duration = Duration::from_secs(300);

    /// Create new IPC endpoint
    pub fn try_new(configuration: ProtocolEndpointConfiguration) -> Result<Self, IpcError> {
        Ok(IpcCmdServer(
            IpcServer::bind_path(&temp_sock())?,
            configuration,
        ))
    }

    /// Start accepting incoming IPC connection.
    ///
    /// Returns a [`protocol controller`](ProtocolController) if new IPC channel is successfully created.
    /// This is a blocking operation.
    pub fn try_accept(&mut self, timeout: Duration) -> Result<ProtocolController, IpcError> {
        let (rx, tx) = self.0.try_accept(timeout)?;
        // configure default IO timeouts
        rx.set_read_timeout(Some(Self::IO_TIMEOUT))
            .and(tx.set_write_timeout(Some(Self::IO_TIMEOUT)))
            .map_err(|err| IpcError::SocketConfigurationError { reason: err })?;

        Ok(ProtocolController {
            io: RefCell::new(IpcIO {
                rx,
                tx,
                communication_balance: 0,
            }),
            configuration: self.1.clone(),
            shutting_down: false,
        })
    }
}

struct IpcIO {
    rx: IpcReceiver<NodeMessage>,
    tx: IpcSender<ProtocolMessage>,
    /// To keep track of sent/received messages.
    /// When sending a message this must be 0, otherwise there are outstanding recvs.
    /// When a message is sent, the balance is increased by 1, and when a message is
    /// received it is decreased by 1.
    communication_balance: i32,
}

impl IpcIO {
    pub fn send(&mut self, value: &ProtocolMessage) -> Result<(), ipc::IpcError> {
        // Don't send anything until pending messages have been discarded
        self.discard_pending_messages(
            Some(ProtocolController::DISCARD_UNRECEIVED_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
            0,
        )?;
        self.tx.send(value)?;
        self.communication_balance += 1;
        Ok(())
    }

    pub fn send_without_discard(&mut self, value: &ProtocolMessage) -> Result<(), ipc::IpcError> {
        self.tx.send(value)?;
        self.communication_balance += 1;
        Ok(())
    }

    pub fn try_receive(
        &mut self,
        read_timeout: Option<Duration>,
        reset_read_timeout: Option<Duration>,
    ) -> Result<NodeMessage, ipc::IpcError> {
        let result = self.rx.try_receive(read_timeout, reset_read_timeout)?;
        self.communication_balance -= 1;
        Ok(result)
    }

    /// Discard any pending message from cancelled requests
    /// If `read_timeout` is reached, this will fail, and this runner should be shut down.
    fn discard_pending_messages(
        &mut self,
        read_timeout: Option<Duration>,
        reset_read_timeout: Option<Duration>,
        keep_count: u16,
    ) -> Result<(), ipc::IpcError> {
        let keep_count = keep_count.into();
        while self.communication_balance > keep_count {
            // If there is another timeout, bailout, because we are probably stuck.
            // Otherwise keep discarding
            match self.rx.try_receive(read_timeout, reset_read_timeout) {
                Err(IpcError::ReceiveMessageTimeout) => {
                    return Err(IpcError::DiscardMessageTimeout)
                }
                Err(_) => (),
                Ok(_) => (),
            };
            self.communication_balance -= 1;
        }
        Ok(())
    }
}

/// Encapsulate IPC communication.
pub struct ProtocolController {
    io: RefCell<IpcIO>,
    configuration: ProtocolEndpointConfiguration,
    /// Indicates that was triggered shutting down
    shutting_down: bool,
}

/// Provides convenience methods for IPC communication.
///
/// Instead of manually sending and receiving messages over IPC channel use provided methods.
/// Methods also handle things such as timeouts and also checks is correct response type is received.
impl ProtocolController {
    const APPLY_BLOCK_TIMEOUT: Duration = Duration::from_secs(60 * 60 * 2);
    const INIT_PROTOCOL_CONTEXT_TIMEOUT: Duration = Duration::from_secs(60);
    const BEGIN_APPLICATION_TIMEOUT: Duration = Duration::from_secs(120);
    const BEGIN_CONSTRUCTION_TIMEOUT: Duration = Duration::from_secs(120);
    const VALIDATE_OPERATION_TIMEOUT: Duration = Duration::from_secs(120);
    const CALL_PROTOCOL_RPC_TIMEOUT: Duration = Duration::from_secs(30);
    const CALL_PROTOCOL_HEAVY_RPC_TIMEOUT: Duration = Duration::from_secs(600);
    const COMPUTE_PATH_TIMEOUT: Duration = Duration::from_secs(30);
    const JSON_ENCODE_DATA_TIMEOUT: Duration = Duration::from_secs(30);
    const ASSERT_ENCODING_FOR_PROTOCOL_DATA_TIMEOUT: Duration = Duration::from_secs(15);
    const DISCARD_UNRECEIVED_TIMEOUT: Duration = Duration::from_secs(5);

    /// Apply block
    pub fn apply_block(
        &self,
        request: ApplyBlockRequest,
    ) -> Result<ApplyBlockResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ApplyBlockCall(request))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::APPLY_BLOCK_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ApplyBlockResult(result) => {
                result.map_err(|err| ProtocolError::ApplyBlockError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn assert_encoding_for_protocol_data(
        &self,
        protocol_hash: ProtocolHash,
        protocol_data: RustBytes,
    ) -> Result<(), ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::AssertEncodingForProtocolDataCall(
            protocol_hash,
            protocol_data,
        ))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::ASSERT_ENCODING_FOR_PROTOCOL_DATA_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::AssertEncodingForProtocolDataResult(result) => result.map_err(|err| {
                ProtocolError::AssertEncodingForProtocolDataError { reason: err }.into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Begin application
    pub fn begin_application(
        &self,
        request: BeginApplicationRequest,
    ) -> Result<BeginApplicationResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::BeginApplicationCall(request))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::BEGIN_APPLICATION_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::BeginApplicationResult(result) => {
                result.map_err(|err| ProtocolError::BeginApplicationError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Begin construction
    pub fn begin_construction(
        &self,
        request: BeginConstructionRequest,
    ) -> Result<PrevalidatorWrapper, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::BeginConstructionCall(request))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::BEGIN_CONSTRUCTION_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::BeginConstructionResult(result) => {
                result.map_err(|err| ProtocolError::BeginConstructionError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Validate operation
    pub fn validate_operation(
        &self,
        request: ValidateOperationRequest,
    ) -> Result<ValidateOperationResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ValidateOperationCall(request))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::VALIDATE_OPERATION_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ValidateOperationResponse(result) => {
                result.map_err(|err| ProtocolError::ValidateOperationError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// ComputePath
    pub fn compute_path(
        &self,
        request: ComputePathRequest,
    ) -> Result<ComputePathResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ComputePathCall(request))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::COMPUTE_PATH_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ComputePathResponse(result) => {
                result.map_err(|err| ProtocolError::ComputePathError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn apply_block_result_metadata(
        &self,
        context_hash: ContextHash,
        metadata_bytes: RustBytes,
        max_operations_ttl: i32,
        protocol_hash: ProtocolHash,
        next_protocol_hash: ProtocolHash,
    ) -> Result<String, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::JsonEncodeApplyBlockResultMetadata(
            JsonEncodeApplyBlockResultMetadataParams {
                context_hash,
                max_operations_ttl,
                metadata_bytes,
                protocol_hash,
                next_protocol_hash,
            },
        ))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::JSON_ENCODE_DATA_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::JsonEncodeApplyBlockResultMetadataResponse(result) => {
                result.map_err(|err| {
                    ProtocolError::FfiJsonEncoderError {
                        caller: "apply_block_result_metadata".to_owned(),
                        reason: err,
                    }
                    .into()
                })
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn apply_block_operations_metadata(
        &self,
        chain_id: ChainId,
        operations: Vec<Vec<Operation>>,
        operations_metadata_bytes: Vec<Vec<RustBytes>>,
        protocol_hash: ProtocolHash,
        next_protocol_hash: ProtocolHash,
    ) -> Result<String, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::JsonEncodeApplyBlockOperationsMetadata(
            JsonEncodeApplyBlockOperationsMetadataParams {
                chain_id,
                operations,
                operations_metadata_bytes,
                protocol_hash,
                next_protocol_hash,
            },
        ))?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::JSON_ENCODE_DATA_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::JsonEncodeApplyBlockOperationsMetadata(result) => result.map_err(|err| {
                ProtocolError::FfiJsonEncoderError {
                    caller: "apply_block_operations_metadata".to_owned(),
                    reason: err,
                }
                .into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Call protocol  rpc - internal
    fn call_protocol_rpc_internal(
        &self,
        request_path: String,
        msg: ProtocolMessage,
    ) -> Result<ProtocolRpcResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&msg)?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::CALL_PROTOCOL_HEAVY_RPC_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::RpcResponse(result) => result.map_err(|err| {
                ProtocolError::ProtocolRpcError {
                    reason: err,
                    request_path,
                }
                .into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Call protocol rpc
    pub fn call_protocol_rpc(
        &self,
        request: ProtocolRpcRequest,
    ) -> Result<ProtocolRpcResponse, ProtocolServiceError> {
        self.call_protocol_rpc_internal(
            request.request.context_path.clone(),
            ProtocolMessage::ProtocolRpcCall(request),
        )
    }

    /// Call helpers_preapply_* shell service - internal
    fn call_helpers_preapply_internal(
        &self,
        msg: ProtocolMessage,
    ) -> Result<HelpersPreapplyResponse, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&msg)?;

        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::CALL_PROTOCOL_RPC_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::HelpersPreapplyResponse(result) => {
                result.map_err(|err| ProtocolError::HelpersPreapplyError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Call helpers_preapply_operations shell service
    pub fn helpers_preapply_operations(
        &self,
        request: ProtocolRpcRequest,
    ) -> Result<HelpersPreapplyResponse, ProtocolServiceError> {
        self.call_helpers_preapply_internal(ProtocolMessage::HelpersPreapplyOperationsCall(request))
    }

    /// Call helpers_preapply_block shell service
    pub fn helpers_preapply_block(
        &self,
        request: HelpersPreapplyBlockRequest,
    ) -> Result<HelpersPreapplyResponse, ProtocolServiceError> {
        self.call_helpers_preapply_internal(ProtocolMessage::HelpersPreapplyBlockCall(request))
    }

    /// Change tezos runtime configuration
    pub fn change_runtime_configuration(
        &self,
        settings: TezosRuntimeConfiguration,
    ) -> Result<(), ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ChangeRuntimeConfigurationCall(settings))?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ChangeRuntimeConfigurationResult(result) => result.map_err(|err| {
                ProtocolError::TezosRuntimeConfigurationError { reason: err }.into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Command tezos ocaml code to initialize context and protocol.
    /// CommitGenesisResult is returned only if commit_genesis is set to true
    fn init_protocol_context(
        &self,
        storage: TezosContextStorageConfiguration,
        tezos_environment: &TezosEnvironmentConfiguration,
        commit_genesis: bool,
        enable_testchain: bool,
        readonly: bool,
        patch_context: Option<PatchContext>,
        context_stats_db_path: Option<PathBuf>,
    ) -> Result<InitProtocolContextResult, ProtocolServiceError> {
        // try to check if was at least one write success, other words, if context was already created on file system
        {
            // lock
            let lock: Arc<(Mutex<bool>, Condvar)> =
                AT_LEAST_ONE_WRITE_PROTOCOL_CONTEXT_WAS_SUCCESS_AT_FIRST_LOCK.clone();
            let &(ref lock, ref cvar) = &*lock;
            let was_one_write_success = lock.lock()?;

            // if we have already one write, we can just continue, if not we put thread to sleep and wait
            if !(*was_one_write_success) {
                if readonly {
                    // release lock here and wait
                    let _lock = cvar.wait(was_one_write_success)?;
                }
                // TODO: handle situation, thah more writes - we cannot allowed to do so, just one write can exists
            }
        }

        // call init
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::InitProtocolContextCall(
            InitProtocolContextParams {
                storage,
                genesis: tezos_environment.genesis.clone(),
                genesis_max_operations_ttl: tezos_environment
                    .genesis_additional_data()
                    .map_err(|error| ProtocolServiceError::InvalidDataError {
                        message: format!("{:?}", error),
                    })?
                    .max_operations_ttl,
                protocol_overrides: tezos_environment.protocol_overrides.clone(),
                commit_genesis,
                enable_testchain,
                readonly,
                turn_off_context_raw_inspector: true, // TODO - TE-261: remove later, new context doesn't use it
                patch_context,
                context_stats_db_path,
            },
        ))?;

        // wait for response
        // this might take a while, so we will use unusually long timeout
        match io.try_receive(
            Some(Self::INIT_PROTOCOL_CONTEXT_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::InitProtocolContextResult(result) => {
                if result.is_ok() {
                    // if context is initialized, and is not readonly, means is write, for wich we wait
                    // we check if it is the first one, if it is the first one, we can notify other threads to continue
                    if !readonly {
                        // check if first write success
                        let lock: Arc<(Mutex<bool>, Condvar)> =
                            AT_LEAST_ONE_WRITE_PROTOCOL_CONTEXT_WAS_SUCCESS_AT_FIRST_LOCK.clone();
                        let &(ref lock, ref cvar) = &*lock;
                        let mut was_one_write_success =
                            lock.lock()
                                .map_err(|error| ProtocolServiceError::LockPoisonError {
                                    message: format!("{:?}", error),
                                })?;
                        if !(*was_one_write_success) {
                            *was_one_write_success = true;
                            cvar.notify_all();
                        }
                    }
                }
                result.map_err(|err| ProtocolError::OcamlStorageInitError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Gracefully shutdown protocol runner
    pub fn shutdown(&mut self) -> Result<(), ProtocolServiceError> {
        if self.shutting_down {
            // shutdown was already triggered before
            return Ok(());
        }
        self.shutting_down = true;

        let mut io = self.io.borrow_mut();

        // For shutdown messages we don't care if there are pending reads
        io.send_without_discard(&ProtocolMessage::ShutdownCall)?;
        io.discard_pending_messages(
            Some(ProtocolController::DISCARD_UNRECEIVED_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
            1,
        )?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ShutdownResult => Ok(()),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    /// Initialize protocol environment from default configuration (writeable).
    pub fn init_protocol_for_write(
        &self,
        commit_genesis: bool,
        patch_context: &Option<PatchContext>,
        context_stats_db_path: Option<PathBuf>,
    ) -> Result<InitProtocolContextResult, ProtocolServiceError> {
        self.change_runtime_configuration(self.configuration.runtime_configuration.clone())?;
        self.init_protocol_context(
            self.configuration.storage.clone(),
            &self.configuration.environment,
            commit_genesis,
            self.configuration.enable_testchain,
            false,
            patch_context.clone(),
            context_stats_db_path,
        )
    }

    /// Initialize protocol environment from default configuration (readonly).
    pub fn init_protocol_for_read(
        &self,
    ) -> Result<InitProtocolContextResult, ProtocolServiceError> {
        // TODO - TE-261: should use a different message exchange for readonly contexts?
        self.change_runtime_configuration(self.configuration.runtime_configuration.clone())?;
        self.init_protocol_context(
            self.configuration.storage.clone(),
            &self.configuration.environment,
            false,
            self.configuration.enable_testchain,
            true,
            None,
            None,
        )
    }

    // TODO - TE-261: this requires more descriptive errors.

    /// Initializes server to listen for readonly context clients through IPC.
    ///
    /// Must be called after the writable context has been initialized.
    pub fn init_context_ipc_server(&self) -> Result<(), ProtocolServiceError> {
        if self.configuration.storage.get_ipc_socket_path().is_some() {
            let mut io = self.io.borrow_mut();
            io.send(&ProtocolMessage::InitProtocolContextIpcServer(
                self.configuration.storage.clone(),
            ))?;

            match io.try_receive(
                Some(IpcCmdServer::IO_TIMEOUT),
                Some(IpcCmdServer::IO_TIMEOUT),
            )? {
                NodeMessage::InitProtocolContextIpcServerResult(result) => {
                    result.map_err(|err| ProtocolServiceError::ContextIpcServerError {
                        message: format!("Failure when starting context IPC server: {}", err),
                    })
                }
                message => Err(ProtocolServiceError::UnexpectedMessage {
                    message: message.into(),
                }),
            }
        } else {
            Ok(())
        }
    }

    /// Gets data for genesis.
    pub fn genesis_result_data(
        &self,
        genesis_context_hash: &ContextHash,
    ) -> Result<CommitGenesisResult, ProtocolServiceError> {
        let tezos_environment = self.configuration.environment.clone();
        let main_chain_id = tezos_environment.main_chain_id().map_err(|e| {
            ProtocolServiceError::InvalidDataError {
                message: format!("{:?}", e),
            }
        })?;
        let protocol_hash = tezos_environment.genesis_protocol().map_err(|e| {
            ProtocolServiceError::InvalidDataError {
                message: format!("{:?}", e),
            }
        })?;

        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::GenesisResultDataCall(
            GenesisResultDataParams {
                genesis_context_hash: genesis_context_hash.clone(),
                chain_id: main_chain_id,
                genesis_protocol_hash: protocol_hash,
                genesis_max_operations_ttl: tezos_environment
                    .genesis_additional_data()
                    .map_err(|error| ProtocolServiceError::InvalidDataError {
                        message: format!("{:?}", error),
                    })?
                    .max_operations_ttl,
            },
        ))?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::CommitGenesisResultData(result) => {
                result.map_err(|err| ProtocolError::GenesisResultDataError { reason: err }.into())
            }
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn get_context_key_from_history(
        &self,
        context_hash: &ContextHash,
        key: ContextKeyOwned,
    ) -> Result<Option<ContextValue>, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ContextGetKeyFromHistory(
            ContextGetKeyFromHistoryRequest {
                context_hash: context_hash.clone(),
                key,
            },
        ))?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT_LONG),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ContextGetKeyFromHistoryResult(result) => result
                .map_err(|err| ProtocolError::ContextGetKeyFromHistoryError { reason: err }.into()),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn get_context_key_values_by_prefix(
        &self,
        context_hash: &ContextHash,
        prefix: ContextKeyOwned,
    ) -> Result<Option<Vec<(ContextKeyOwned, ContextValue)>>, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ContextGetKeyValuesByPrefix(
            ContextGetKeyValuesByPrefixRequest {
                context_hash: context_hash.clone(),
                prefix,
            },
        ))?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT_LONG),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ContextGetKeyValuesByPrefixResult(result) => result.map_err(|err| {
                ProtocolError::ContextGetKeyValuesByPrefixError { reason: err }.into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }

    pub fn get_context_tree_by_prefix(
        &self,
        context_hash: &ContextHash,
        prefix: ContextKeyOwned,
        depth: Option<usize>,
    ) -> Result<StringTreeObject, ProtocolServiceError> {
        let mut io = self.io.borrow_mut();
        io.send(&ProtocolMessage::ContextGetTreeByPrefix(
            ContextGetTreeByPrefixRequest {
                context_hash: context_hash.clone(),
                prefix,
                depth,
            },
        ))?;

        match io.try_receive(
            Some(IpcCmdServer::IO_TIMEOUT_LONG),
            Some(IpcCmdServer::IO_TIMEOUT),
        )? {
            NodeMessage::ContextGetTreeByPrefixResult(result) => result.map_err(|err| {
                ProtocolError::ContextGetKeyValuesByPrefixError { reason: err }.into()
            }),
            message => Err(ProtocolServiceError::UnexpectedMessage {
                message: message.into(),
            }),
        }
    }
}

impl Drop for ProtocolController {
    fn drop(&mut self) {
        // try to gracefully shutdown protocol runner
        if !self.shutting_down {
            let _ = self.shutdown();
        }
    }
}

/// Endpoint consists of a protocol runner and IPC communication (command and event channels).
pub struct ProtocolRunnerEndpoint {
    runner: ExecutableProtocolRunner,
    log: Logger,

    pub commands: IpcCmdServer,
}

impl ProtocolRunnerEndpoint {
    pub fn try_new(
        endpoint_name: &str,
        configuration: ProtocolEndpointConfiguration,
        tokio_runtime: tokio::runtime::Handle,
        log: Logger,
    ) -> Result<ProtocolRunnerEndpoint, IpcError> {
        let cmd_server = IpcCmdServer::try_new(configuration.clone())?;

        Ok(ProtocolRunnerEndpoint {
            runner: ExecutableProtocolRunner::new(
                configuration,
                cmd_server.0.client().path(),
                endpoint_name.to_string(),
                tokio_runtime,
            ),
            commands: cmd_server,
            log,
        })
    }

    /// Starts protocol runner sub-process just once and you can take care of it
    pub fn start(&self) -> Result<tokio::process::Child, ProtocolRunnerError> {
        debug!(self.log, "Starting protocol runner process");
        self.runner.spawn(self.log.clone())
    }
}
